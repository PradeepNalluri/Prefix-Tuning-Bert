{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import  CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARCBertClassifier(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Classifier to handle classification task on SARC dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super(SARCBertClassifier, self).__init__(config)\n",
    "#         self.mlp_layer=None\n",
    "#         self.prefix_embeddings =None\n",
    "        self.run_device = None\n",
    "    def update_network_sarc(self,num_layers,device,freeze_bert_layers=False,custom_embedding=False,custom_embedding_vector=None,add_user_information=False):\n",
    "        \"\"\"\n",
    "        Update the network architecture all the variable are class variables from source code of BerforSequenceClassification\n",
    "        transformer module\n",
    "        \"\"\"\n",
    "        config=self.config\n",
    "        if(freeze_bert_layers):\n",
    "            for name,param in self.bert.named_parameters():\n",
    "                if(name!=\"embeddings.prefix_embeddings.weight\"):\n",
    "                    param.requires_grad = False\n",
    "        self.prefix_embeddings = nn.Embedding(config.prefix_length, config.hidden_size)\n",
    "        self.prefix_length = config.prefix_length\n",
    "        self.mlp_layer = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(config.hidden_size,config.hidden_size))\n",
    "        \n",
    "        if(add_user_information):\n",
    "            self.classifier = nn.Linear(config.hidden_size+2, config.num_labels)\n",
    "        \n",
    "        if(custom_embedding):\n",
    "            self.prefix_length = config.prefix_length\n",
    "            self.mlp_layer = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(config.hidden_size,config.hidden_size))\n",
    "\n",
    "            self.init_weights()\n",
    "            \n",
    "            custom_embedding_vector = custom_embedding_vector.expand(config.prefix_length,custom_embedding_vector.shape[0])\n",
    "            self.prefix_embeddings=nn.Embedding.from_pretrained(custom_embedding_vector)\n",
    "        else:\n",
    "            self.prefix_embeddings = nn.Embedding(config.prefix_length, config.hidden_size)\n",
    "            self.prefix_length = config.prefix_length\n",
    "            self.mlp_layer = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(config.hidden_size,config.hidden_size))\n",
    "            self.init_weights()\n",
    "        self.run_device = device\n",
    "    \n",
    "    def check_closest_matching_bert_model(self):\n",
    "        prefix_tokens = self.prefix_embeddings(torch.LongTensor(torch.arange(1)).to(self.run_device)).detach()\n",
    "        bert_base = self.bert.embeddings.word_embeddings(torch.LongTensor(torch.arange(30522)).to(self.run_device)).detach()\n",
    "        closest_words_ids = [] \n",
    "        for embd in prefix_tokens:\n",
    "            closest_words_ids.append(torch.norm(bert_base - embd.unsqueeze(0), dim=1).topk(5).indices)\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"./prefix_tuning_model_random_initializations_prefix_tuninglr_2e-5/\")\n",
    "        closest_words_ids=torch.stack(closest_words_ids)\n",
    "        closest = {}\n",
    "        for idx,t in enumerate(closest_words_ids):\n",
    "            word_l = []\n",
    "            for tok in t:\n",
    "                word_l.append(tokenizer._convert_id_to_token(int(tok)))\n",
    "            closest[idx]=word_l\n",
    "        return closest\n",
    "    \n",
    "    def closest_matching_bert_model(self):\n",
    "        prefix_tokens = self.prefix_embeddings(torch.LongTensor(torch.arange(self.prefix_embeddings.weight.shape[0])).to(self.run_device)).detach()\n",
    "        bert_base = self.bert.embeddings.word_embeddings(torch.LongTensor(torch.arange(30522)).to(self.run_device)).detach()\n",
    "        closest_words_ids = [] \n",
    "        for embd in prefix_tokens:\n",
    "            closest_words_ids.append(torch.norm(bert_base - embd.unsqueeze(0), dim=1).topk(5).indices)\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"./prefix_tuning_model_random_initializations_prefix_tuninglr_2e-5/\")\n",
    "        closest_words_ids=torch.stack(closest_words_ids)\n",
    "        closest = {}\n",
    "        for idx,t in enumerate(closest_words_ids):\n",
    "            word_l = []\n",
    "            for tok in t:\n",
    "                word_l.append(tokenizer._convert_id_to_token(int(tok)))\n",
    "            closest[idx]=word_l\n",
    "        return closest\n",
    "    \n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,\n",
    "        head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,\n",
    "                user_information=False):\n",
    "        r\"\"\"\n",
    "\n",
    "        FROM CORE HUGGINGFACE MODULE \n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        prefix_embds = self.prefix_embeddings(torch.arange(0, self.prefix_length).to(self.run_device))\n",
    "        prefix_embds = self.mlp_layer(prefix_embds)\n",
    "        prefix_embds = prefix_embds.expand(len(input_ids),prefix_embds.shape[0],prefix_embds.shape[1])\n",
    "        attention_mask = torch.cat((torch.ones(self.prefix_length).to(self.run_device).expand(attention_mask.shape[0],self.prefix_length),attention_mask),1)\n",
    "        \n",
    "#         if(user_information):\n",
    "#             attention_mask = attention_mask[:,2:] \n",
    "#             user_ids =  input_ids[:,:2]\n",
    "#             input_ids = input_ids[:,2:].to(self.run_device).long()\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            prefix_embeddings=prefix_embds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "#         if(user_information):\n",
    "#             pooled_output = torch.cat((user_ids,pooled_output),dim=1)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Embeddings\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "def get_bert_embedding(word):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_res = tokenizer.encode_plus(\n",
    "                            word,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 5,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "    embed_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    embed_model.eval()\n",
    "    embedding = embed_model(**tokenized_res)\n",
    "    embedding = embedding.last_hidden_state.detach()#[:,tokenized_res['attention_mask']]    \n",
    "    required_embedding = embedding[0][tokenized_res['attention_mask'][0]==1,:]\n",
    "    required_embedding = required_embedding.mean(dim=0)\n",
    "    del embed_model\n",
    "    del embedding \n",
    "    return required_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embedding = get_bert_embedding(\"sarcasm detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "# config.prefix_length = 5\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# model = SARCBertClassifier(config)\n",
    "\n",
    "# model.update_network_sarc(2,device,freeze_bert_layers=True)\n",
    "\n",
    "# # model = nn.DataParallel(model)\n",
    "# model.to(device)\n",
    "# model.cuda()\n",
    "\n",
    "# for n,p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(n)\n",
    "device = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_include = False\n",
    "experiment = \"noprefix_bottomtwo_layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: User Embeddings Exception: 'BertConfig' object has no attribute 'prefix_length'\n",
      "Tuning: bert.encoder.layer.0.attention.self.query.weight\n",
      "Tuning: bert.encoder.layer.0.attention.self.query.bias\n",
      "Tuning: bert.encoder.layer.0.attention.self.key.weight\n",
      "Tuning: bert.encoder.layer.0.attention.self.key.bias\n",
      "Tuning: bert.encoder.layer.0.attention.self.value.weight\n",
      "Tuning: bert.encoder.layer.0.attention.self.value.bias\n",
      "Tuning: bert.encoder.layer.0.attention.output.dense.weight\n",
      "Tuning: bert.encoder.layer.0.attention.output.dense.bias\n",
      "Tuning: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Tuning: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Tuning: bert.encoder.layer.0.intermediate.dense.weight\n",
      "Tuning: bert.encoder.layer.0.intermediate.dense.bias\n",
      "Tuning: bert.encoder.layer.0.output.dense.weight\n",
      "Tuning: bert.encoder.layer.0.output.dense.bias\n",
      "Tuning: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "Tuning: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "Tuning: bert.encoder.layer.1.attention.self.query.weight\n",
      "Tuning: bert.encoder.layer.1.attention.self.query.bias\n",
      "Tuning: bert.encoder.layer.1.attention.self.key.weight\n",
      "Tuning: bert.encoder.layer.1.attention.self.key.bias\n",
      "Tuning: bert.encoder.layer.1.attention.self.value.weight\n",
      "Tuning: bert.encoder.layer.1.attention.self.value.bias\n",
      "Tuning: bert.encoder.layer.1.attention.output.dense.weight\n",
      "Tuning: bert.encoder.layer.1.attention.output.dense.bias\n",
      "Tuning: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Tuning: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Tuning: bert.encoder.layer.1.intermediate.dense.weight\n",
      "Tuning: bert.encoder.layer.1.intermediate.dense.bias\n",
      "Tuning: bert.encoder.layer.1.output.dense.weight\n",
      "Tuning: bert.encoder.layer.1.output.dense.bias\n",
      "Tuning: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "Tuning: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "Tuning: classifier.weight\n"
     ]
    }
   ],
   "source": [
    "if(prefix_include):\n",
    "    model = SARCBertClassifier(config)\n",
    "\n",
    "    model.update_network_sarc(2,device,freeze_bert_layers=True)#,custom_embedding=True,custom_embedding_vector=custom_embedding)\n",
    "    \n",
    "else:\n",
    "    model = BertForSequenceClassification(config)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for n,p in model.named_parameters():\n",
    "    if(experiment == \"noprefix_toptwo_layers\"):\n",
    "        if(\"bert.encoder.layer.10.\" in n or \"bert.encoder.layer.11.\" in n or n==\"classifier.weight\"):\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "        if p.requires_grad:\n",
    "            print(\"Tuning:\",n)\n",
    "    if(experiment == \"noprefix_bottomtwo_layers\"):\n",
    "        if(\"bert.encoder.layer.0.\" in n or \"bert.encoder.layer.1.\" in n or n==\"classifier.weight\"):\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "        if p.requires_grad:\n",
    "            print(\"Tuning:\",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader_train))\n",
    "yhat = model(b_input_ids,b_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(yhat['logits'], params=dict(list(model.named_parameters()))).render(\"bert_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818,723 training samples\n",
      "90,970 validation samples\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.58: 100%|██████████| 12793/12793 [55:27<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:55:28\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1422/1422 [02:50<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.71\n",
      "  Validation Loss: 0.56\n",
      "  Validation took: 0:02:51\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.55: 100%|██████████| 12793/12793 [55:24<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epcoh took: 0:55:25\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1422/1422 [02:50<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.72\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:02:51\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.53:  45%|████▌     | 5802/12793 [25:10<30:19,  3.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-83c89dac735e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# This is to help prevent the \"exploding gradients\" problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Update parameters and take a step using the computed gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/home/pnalluri/.conda/envs/prefix-env/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parallelize(function_pointer,list_to_parallelize,NUM_CORE=2*mp.cpu_count()):\n",
    "    '''\n",
    "    Prallel apply the given function to the list the numeber of process will \n",
    "    be twice the number of cpu cores by default \n",
    "    '''\n",
    "    start=time.time()\n",
    "    component_list=np.array_split(list_to_parallelize,NUM_CORE*10)\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    results = pool.map(function_pointer,component_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end=time.time()\n",
    "    print(\"Executed in:\",end-start)\n",
    "    return results\n",
    "\n",
    "\n",
    "import pickle\n",
    "# f = open('processed_data.pckl', 'wb')\n",
    "# pickle.dump([input_ids,attention_masks,labels], f)\n",
    "# f.close()\n",
    "\n",
    "f = open('processed_data.pckl', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "input_ids = test[0]\n",
    "attention_masks = test[1]\n",
    "labels = test[2]\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            )\n",
    "\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats_custom = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "# total_t0 = time.time()\n",
    "import json\n",
    "import os\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    \n",
    "    total_training_loss = 0\n",
    "    total_correct_predictions, total_predictions = 0, 0\n",
    "    generator_tqdm = tqdm(train_dataloader)\n",
    "    output_dir = './NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    elif(epoch_i==0):\n",
    "        print(\"directory exists\")\n",
    "        raise Exception('My error!')\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     with open(output_dir+\"prefix_embed_matching_words_epoch_\"+str(epoch_i)+\".json\", 'w') as fp:\n",
    "#         json.dump(model.closest_matching_bert_model(), fp)\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(generator_tqdm):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if(not prefix_include):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "        else:\n",
    "            b_input_ids = torch.cat((torch.arange(0, config.prefix_length).expand(batch[0].shape[0], config.prefix_length),batch[0]),1).to(device)\n",
    "            b_input_mask = torch.cat((torch.ones(config.prefix_length).expand(batch[1].shape[0], config.prefix_length),batch[1]),1).to(device)\n",
    "        \n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "#         optimizer.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.mean()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.mean().backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "#         batch_predictions = np.argmax(nn.Softmax(dim=1)(logits).detach().cpu().numpy(), axis=-1)\n",
    "#         total_correct_predictions += (batch_predictions == b_labels.detach().cpu().numpy()).sum()\n",
    "#         total_predictions += b_labels.shape[0]\n",
    "            \n",
    "        description = (\"Average training loss: %.2f\" % (total_train_loss/(step+1)))\n",
    "        generator_tqdm.set_description(description, refresh=False)\n",
    "        \n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    print(\"Evaluation In Progress\")\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        if(prefix_include):\n",
    "            b_input_ids = torch.cat((torch.arange(0, config.prefix_length).expand(batch[0].shape[0], config.prefix_length),batch[0]),1).to(device)\n",
    "\n",
    "            b_input_mask = torch.cat((torch.ones(config.prefix_length).expand(batch[1].shape[0], config.prefix_length),batch[1]),1).to(device)\n",
    "        else:\n",
    "            b_input_ids=batch[0].to(device)\n",
    "            b_input_mask=batch[1].to(device) \n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.mean()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats_custom.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': tensor(0.6084, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.5761, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.6918512658227848,\n",
       "  'Training Time': '0:34:47',\n",
       "  'Validation Time': '4:25:36'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': tensor(0.5818, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.5691, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7037335686465434,\n",
       "  'Training Time': '0:34:42',\n",
       "  'Validation Time': '0:02:51'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': tensor(0.5691, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.5574, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.711689758465866,\n",
       "  'Training Time': '0:34:44',\n",
       "  'Validation Time': '0:02:51'},\n",
       " {'epoch': 4,\n",
       "  'Training Loss': tensor(0.5602, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.5556, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7143590081683436,\n",
       "  'Training Time': '0:34:46',\n",
       "  'Validation Time': '0:02:51'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prefix_embeddings(torch.Tensor(1).long().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prefix_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.ones(5).expand(128, 5),batch[1]),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0., 5).expand(128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=torch.cat((torch.arange(0., 5).expand(128, 5),batch[0]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in generator_tqdm:#\n",
    "    print(b[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.get_parameter('embeddings.prefix_embeddings.weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-03T23:10:00.105143Z",
     "iopub.status.busy": "2021-11-03T23:10:00.104550Z",
     "iopub.status.idle": "2021-11-03T23:10:00.136108Z",
     "shell.execute_reply": "2021-11-03T23:10:00.135218Z",
     "shell.execute_reply.started": "2021-11-03T23:10:00.105050Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parallelize(function_pointer,list_to_parallelize,NUM_CORE=2*mp.cpu_count()):\n",
    "    '''\n",
    "    Prallel apply the given function to the list the numeber of process will \n",
    "    be twice the number of cpu cores by default \n",
    "    '''\n",
    "    start=time.time()\n",
    "    component_list=np.array_split(list_to_parallelize,NUM_CORE*10)\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    results = pool.map(function_pointer,component_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end=time.time()\n",
    "    print(\"Executed in:\",end-start)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:10:00.138594Z",
     "iopub.status.busy": "2021-11-03T23:10:00.137753Z",
     "iopub.status.idle": "2021-11-03T23:10:07.471293Z",
     "shell.execute_reply": "2021-11-03T23:10:07.470368Z",
     "shell.execute_reply.started": "2021-11-03T23:10:00.138547Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set,test_set = train_test_split(data,stratify=data[[\"label\"]], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv(\"test_set.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.dropna(subset=[\"comment\"],inplace=True)\n",
    "\n",
    "training_set.reset_index(drop=False,inplace=True)\n",
    "\n",
    "training_set.rename(columns={\"index\":\"id\"},inplace=True)\n",
    "sentences = training_set[[\"id\",\"comment\"]]\n",
    "labels = training_set[[\"id\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:11:52.688551Z",
     "iopub.status.busy": "2021-11-03T23:11:52.688207Z",
     "iopub.status.idle": "2021-11-03T23:12:02.170611Z",
     "shell.execute_reply": "2021-11-03T23:12:02.169858Z",
     "shell.execute_reply.started": "2021-11-03T23:11:52.688514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:13:58.195115Z",
     "iopub.status.busy": "2021-11-03T23:13:58.194542Z",
     "iopub.status.idle": "2021-11-03T23:14:17.897113Z",
     "shell.execute_reply": "2021-11-03T23:14:17.896258Z",
     "shell.execute_reply.started": "2021-11-03T23:13:58.195076Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    max_len = 0\n",
    "    for _,row in sentences.iterrows():\n",
    "        sent=row[\"comment\"]\n",
    "        try:\n",
    "            input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        except:\n",
    "            input_ids = tokenizer.encode(\"\", add_special_tokens=True)\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = max(parallelize(tokenize,sentences))\n",
    "# tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=training_set.comment.values\n",
    "labels = training_set.label.values\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# f = open('processed_data.pckl', 'wb')\n",
    "# pickle.dump([input_ids,attention_masks,labels], f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('processed_data.pckl', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = test[0]\n",
    "attention_masks = test[1]\n",
    "labels = test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 128\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in validation_dataloader:\n",
    "    print(len(i))\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import  CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "class SARCBertClassifier(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Classifier to handle classification task on SARC dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super(SARCBertClassifier, self).__init__(config)\n",
    "        \n",
    "    def update_network_sarc(self,num_layers,device,freeze_bert_layers=False,training_mode=\"PrefixTuning\",sequence_length=3,embedding_dimension=1024):\n",
    "        \"\"\"\n",
    "        Update the network architecture all the variable are class variables from source code of BerforSequenceClassification\n",
    "        transformer module\n",
    "        \"\"\"\n",
    "        config=self.config\n",
    "        if(freeze_bert_layers):\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.training_mode = training_mode\n",
    "        if(self.training_mode==\"PrefixTuning\"):\n",
    "            self.prefix_layer = nn.Embedding(sequence_length,embedding_dimension)\n",
    "    \n",
    "        self.classifier = nn.Sequential()\n",
    "        for layer in range(num_layers-1):\n",
    "            self.classifier.add_module(\"classification_layer_\"+str(layer+1),nn.Linear(config.hidden_size, config.hidden_size))\n",
    "            self.classifier.add_module(\"activation_layer_\"+str(layer+1),nn.ReLU())    \n",
    "        \n",
    "        self.classifier.add_module(\"output_layer\",nn.Linear(config.hidden_size, config.num_labels))\n",
    "        self.classifier.to(device)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,\n",
    "        head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,):\n",
    "        r\"\"\"\n",
    "\n",
    "        FROM CORE HUGGINGFACE MODULE \n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "custom=True\n",
    "if custom:\n",
    "    model = SARCBertClassifier.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    model.update_network_sarc(2,device,freeze_bert_layers=True)\n",
    "else:\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification. \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 0.05, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats_custom = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    \n",
    "    total_training_loss = 0\n",
    "    total_correct_predictions, total_predictions = 0, 0\n",
    "    generator_tqdm = tqdm(train_dataloader)\n",
    "        \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(generator_tqdm):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "#         optimizer.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.mean()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.mean().backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "        batch_predictions = np.argmax(nn.Softmax(dim=1)(logits).detach().cpu().numpy(), axis=-1)\n",
    "        total_correct_predictions += (batch_predictions == b_labels.detach().cpu().numpy()).sum()\n",
    "        total_predictions += b_labels.shape[0]\n",
    "            \n",
    "        description = (\"Average training loss: %.2f Accuracy: %.2f  Lable sum: %2f\"\n",
    "                           % (total_train_loss/(step+1), total_correct_predictions/total_predictions,batch_predictions.sum()))\n",
    "        generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    print(\"Evaluation In Progress\")\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.mean()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats_custom.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = training_stats_custom\n",
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "# df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2,3,4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./prefix_tuning_model_random_initializations/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/tokenizer_config.json',\n",
       " './NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/special_tokens_map.json',\n",
       " './NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/vocab.txt',\n",
       " './NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './NO_prefix_tuning_with_unfreezed_bottom_2_layers_random_intialization/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "output_dir = './prefix_tuning_with_unfreezed_embedding_layer/'\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(output_dir,)\n",
    "config.user_embeddings=True\n",
    "\n",
    "model = SARCBertClassifier.from_pretrained(output_dir)\n",
    "# tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 101,083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.dropna(subset=[\"comment\"]).comment.values\n",
    "labels = df.dropna(subset=[\"comment\"]).label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101080/101080 [00:04<00:00, 22753.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pad our input tokens\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in tqdm(input_ids):\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 101,080 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3159/3159 [03:15<00:00, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    if(prefix_include):\n",
    "        b_input_ids = torch.cat((torch.arange(0, 5).expand(b_input_ids.shape[0], 5).to(device),b_input_ids),1).to(device)\n",
    "        b_input_mask = torch.cat((torch.ones(5).expand(b_input_mask.shape[0], 5).to(device),b_input_mask),1).to(device)\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "    logits = outputs.logits#outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14177280"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82481015,  1.2320392 ],\n",
       "       [-1.6283038 ,  1.9350392 ],\n",
       "       [-0.36508894,  0.7768754 ],\n",
       "       [ 0.5367286 , -0.3059828 ],\n",
       "       [ 0.9837166 , -0.8803557 ],\n",
       "       [ 0.6849129 , -0.46184498],\n",
       "       [ 0.4050085 , -0.1421056 ],\n",
       "       [ 0.8203619 , -0.65921706],\n",
       "       [ 0.06800893,  0.29526252],\n",
       "       [-1.3281451 ,  1.6307966 ],\n",
       "       [ 0.37128958, -0.05452841],\n",
       "       [ 0.5075836 , -0.26008534],\n",
       "       [ 0.64459616, -0.38483444],\n",
       "       [ 0.7153135 , -0.48924652],\n",
       "       [-0.04507451,  0.43352556],\n",
       "       [ 0.6061663 , -0.35672152],\n",
       "       [ 0.35888046, -0.0669265 ],\n",
       "       [-0.3528226 ,  0.7524141 ],\n",
       "       [-0.46195805,  0.84969926],\n",
       "       [ 1.1890585 , -1.2155231 ],\n",
       "       [-0.3090629 ,  0.8012265 ],\n",
       "       [ 0.8838628 , -0.7491174 ],\n",
       "       [-0.7050549 ,  1.0997629 ],\n",
       "       [ 0.26810032,  0.03972005],\n",
       "       [-2.1440077 ,  2.3286133 ],\n",
       "       [ 0.22543845,  0.11010227],\n",
       "       [ 0.7372776 , -0.5644066 ],\n",
       "       [ 0.69390297, -0.47931266],\n",
       "       [ 0.45808038, -0.16729285],\n",
       "       [ 0.9348411 , -0.84890056],\n",
       "       [-1.0685457 ,  1.4393041 ],\n",
       "       [-0.36985278,  0.7869933 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "actuals=[]\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "    preds.append(list(np.argmax(predictions[i], axis=1).flatten()))\n",
    "    actuals.append(list(true_labels[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "preds = list(itertools.chain(*preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = list(itertools.chain(*actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.61357340720221"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(actuals,preds)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7159823497761157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Macro F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(actuals, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from string import ascii_uppercase\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confm = confusion_matrix(actuals, preds,normalize='true')\n",
    "\n",
    "columns = [\"Sarcastic\",\"Normal\"]\n",
    "\n",
    "\n",
    "df_cm = DataFrame(confm, index=columns, columns=columns)\n",
    "\n",
    "ax = sn.heatmap(df_cm, cmap='Oranges', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(preds)/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefix-env",
   "language": "python",
   "name": "prefix-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
