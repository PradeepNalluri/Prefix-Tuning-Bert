{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-03T23:10:00.105143Z",
     "iopub.status.busy": "2021-11-03T23:10:00.104550Z",
     "iopub.status.idle": "2021-11-03T23:10:00.136108Z",
     "shell.execute_reply": "2021-11-03T23:10:00.135218Z",
     "shell.execute_reply.started": "2021-11-03T23:10:00.105050Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parallelize(function_pointer,list_to_parallelize,NUM_CORE=2*mp.cpu_count()):\n",
    "    '''\n",
    "    Prallel apply the given function to the list the numeber of process will \n",
    "    be twice the number of cpu cores by default \n",
    "    '''\n",
    "    start=time.time()\n",
    "    component_list=np.array_split(list_to_parallelize,NUM_CORE*10)\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    results = pool.map(function_pointer,component_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end=time.time()\n",
    "    print(\"Executed in:\",end-start)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:10:00.138594Z",
     "iopub.status.busy": "2021-11-03T23:10:00.137753Z",
     "iopub.status.idle": "2021-11-03T23:10:07.471293Z",
     "shell.execute_reply": "2021-11-03T23:10:07.470368Z",
     "shell.execute_reply.started": "2021-11-03T23:10:00.138547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set,test_set = train_test_split(data,stratify=data[[\"label\"]], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv(\"test_set.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.dropna(subset=[\"comment\"],inplace=True)\n",
    "\n",
    "training_set.reset_index(drop=False,inplace=True)\n",
    "\n",
    "training_set.rename(columns={\"index\":\"id\"},inplace=True)\n",
    "sentences = training_set[[\"id\",\"comment\"]]\n",
    "labels = training_set[[\"id\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:11:52.688551Z",
     "iopub.status.busy": "2021-11-03T23:11:52.688207Z",
     "iopub.status.idle": "2021-11-03T23:12:02.170611Z",
     "shell.execute_reply": "2021-11-03T23:12:02.169858Z",
     "shell.execute_reply.started": "2021-11-03T23:11:52.688514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T23:13:58.195115Z",
     "iopub.status.busy": "2021-11-03T23:13:58.194542Z",
     "iopub.status.idle": "2021-11-03T23:14:17.897113Z",
     "shell.execute_reply": "2021-11-03T23:14:17.896258Z",
     "shell.execute_reply.started": "2021-11-03T23:13:58.195076Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    max_len = 0\n",
    "    for _,row in sentences.iterrows():\n",
    "        sent=row[\"comment\"]\n",
    "        try:\n",
    "            input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        except:\n",
    "            input_ids = tokenizer.encode(\"\", add_special_tokens=True)\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4006 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2224 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1334 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2466 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2310 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2002 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed in: 52.346325635910034\n"
     ]
    }
   ],
   "source": [
    "max_len = max(parallelize(tokenize,sentences))\n",
    "# tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/909693 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/gpfs/home/pnalluri/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 909693/909693 [07:02<00:00, 2151.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  But let's keep doing it anyway, just in case\n",
      "Token IDs: tensor([ 101, 2021, 2292, 1005, 1055, 2562, 2725, 2009, 4312, 1010, 2074, 1999,\n",
      "        2553,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "sentences=training_set.comment.values\n",
    "labels = training_set.label.values\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('processed_data.pckl', 'wb')\n",
    "pickle.dump([input_ids,attention_masks,labels], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('processed_data.pckl', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = test[0]\n",
    "attention_masks = test[1]\n",
    "labels = test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818,723 training samples\n",
      "90,970 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 128\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "for i in validation_dataloader:\n",
    "    print(len(i))\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "MAIN_MODEL = copy.deepcopy(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM BEGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "tt = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tt.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import  CrossEntropyLoss\n",
    "class SARCBertClassifier(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Classifier to handle classification task on SARC dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super(SARCBertClassifier, self).__init__(config)\n",
    "        \n",
    "    def update_network_sarc(self,num_layers,device,freeze_bert_layers=False):\n",
    "        config=self.config\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Sequential()\n",
    "        for layer in range(num_layers-1):\n",
    "            self.classifier.add_module(\"classification_layer_\"+str(layer),nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        self.classifier.add_module(\"output_layer\",nn.Linear(config.hidden_size, config.num_labels))\n",
    "        self.classifier.to(device)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,\n",
    "    output_attentions=None,output_hidden_states=None,return_dict=None,):\n",
    "        \"\"\"\n",
    "        FROM THE TRANSFORMER BERTFORSEQUENCECLASSIFICATION MODULE\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,head_mask=head_mask,inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,output_hidden_states=output_hidden_states,return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SARCBertClassifier: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SARCBertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SARCBertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SARCBertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SARCBertClassifier.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_network_sarc(0,device,freeze_bert_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): SARCBertClassifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Sequential(\n",
       "      (output_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-ae20cac00afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm2' is not defined"
     ]
    }
   ],
   "source": [
    "del m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6397 [00:00<?, ?it/s]/gpfs/home/pnalluri/.conda/envs/bert-env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 33%|███▎      | 2141/6397 [12:48<25:18,  2.80it/s]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats_custom = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "#         print(loss.mean())\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.mean()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.sum().backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    print(\"Evaluation In Progress\")\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.mean()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats_custom.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.DataParallel(m2)\n",
    "model.to(device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "module.bert.embeddings.word_embeddings.weight           (30522, 768)\n",
      "module.bert.embeddings.position_embeddings.weight         (512, 768)\n",
      "module.bert.embeddings.token_type_embeddings.weight         (2, 768)\n",
      "module.bert.embeddings.LayerNorm.weight                       (768,)\n",
      "module.bert.embeddings.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "module.bert.encoder.layer.0.attention.self.query.weight   (768, 768)\n",
      "module.bert.encoder.layer.0.attention.self.query.bias         (768,)\n",
      "module.bert.encoder.layer.0.attention.self.key.weight     (768, 768)\n",
      "module.bert.encoder.layer.0.attention.self.key.bias           (768,)\n",
      "module.bert.encoder.layer.0.attention.self.value.weight   (768, 768)\n",
      "module.bert.encoder.layer.0.attention.self.value.bias         (768,)\n",
      "module.bert.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "module.bert.encoder.layer.0.attention.output.dense.bias       (768,)\n",
      "module.bert.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "module.bert.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "module.bert.encoder.layer.0.intermediate.dense.weight    (3072, 768)\n",
      "module.bert.encoder.layer.0.intermediate.dense.bias          (3072,)\n",
      "module.bert.encoder.layer.0.output.dense.weight          (768, 3072)\n",
      "module.bert.encoder.layer.0.output.dense.bias                 (768,)\n",
      "module.bert.encoder.layer.0.output.LayerNorm.weight           (768,)\n",
      "module.bert.encoder.layer.0.output.LayerNorm.bias             (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "module.bert.pooler.dense.weight                           (768, 768)\n",
      "module.bert.pooler.dense.bias                                 (768,)\n",
      "module.classifier.weight                                    (2, 768)\n",
      "module.classifier.bias                                          (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6397 [00:00<?, ?it/s]/gpfs/home/pnalluri/.conda/envs/bert-env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 6397/6397 [1:03:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epcoh took: 1:03:14\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [02:38<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.47\n",
      "  Validation took: 0:02:39\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6397/6397 [1:03:30<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 1:03:31\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [02:38<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.78\n",
      "  Validation Loss: 0.47\n",
      "  Validation took: 0:02:38\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6397/6397 [1:03:26<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 1:03:27\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [02:38<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.78\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 0:02:39\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6397/6397 [1:03:29<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 1:03:30\n",
      "\n",
      "Running Validation...\n",
      "Evaluation In Progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [02:38<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.51\n",
      "  Validation took: 0:02:39\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:24:16 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "#         print(loss.mean())\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.mean()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.sum().backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    print(\"Evaluation In Progress\")\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.mean()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': tensor(0.4809, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.4748, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7732106579152992,\n",
       "  'Training Time': '1:03:14',\n",
       "  'Validation Time': '0:02:39'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': tensor(0.4327, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.4747, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7755840756368182,\n",
       "  'Training Time': '1:03:31',\n",
       "  'Validation Time': '0:02:38'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': tensor(0.3830, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.4878, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7772449796843257,\n",
       "  'Training Time': '1:03:27',\n",
       "  'Validation Time': '0:02:39'},\n",
       " {'epoch': 4,\n",
       "  'Training Loss': tensor(0.3441, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'Valid. Loss': tensor(0.5139, device='cuda:0'),\n",
       "  'Valid. Accur.': 0.7733771878418504,\n",
       "  'Training Time': '1:03:30',\n",
       "  'Validation Time': '0:02:39'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.4809, device='cuda:0', grad_fn=&lt;DivBa...</td>\n",
       "      <td>1:03:14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>tensor(0.4748, device='cuda:0')</td>\n",
       "      <td>0:02:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.4327, device='cuda:0', grad_fn=&lt;DivBa...</td>\n",
       "      <td>1:03:31</td>\n",
       "      <td>0.78</td>\n",
       "      <td>tensor(0.4747, device='cuda:0')</td>\n",
       "      <td>0:02:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.3830, device='cuda:0', grad_fn=&lt;DivBa...</td>\n",
       "      <td>1:03:27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>tensor(0.4878, device='cuda:0')</td>\n",
       "      <td>0:02:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.3441, device='cuda:0', grad_fn=&lt;DivBa...</td>\n",
       "      <td>1:03:30</td>\n",
       "      <td>0.77</td>\n",
       "      <td>tensor(0.5139, device='cuda:0')</td>\n",
       "      <td>0:02:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Training Loss Training Time  \\\n",
       "epoch                                                                    \n",
       "1      tensor(0.4809, device='cuda:0', grad_fn=<DivBa...       1:03:14   \n",
       "2      tensor(0.4327, device='cuda:0', grad_fn=<DivBa...       1:03:31   \n",
       "3      tensor(0.3830, device='cuda:0', grad_fn=<DivBa...       1:03:27   \n",
       "4      tensor(0.3441, device='cuda:0', grad_fn=<DivBa...       1:03:30   \n",
       "\n",
       "       Valid. Accur.                      Valid. Loss Validation Time  \n",
       "epoch                                                                  \n",
       "1               0.77  tensor(0.4748, device='cuda:0')         0:02:39  \n",
       "2               0.78  tensor(0.4747, device='cuda:0')         0:02:38  \n",
       "3               0.78  tensor(0.4878, device='cuda:0')         0:02:39  \n",
       "4               0.77  tensor(0.5139, device='cuda:0')         0:02:39  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "# df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save_new/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save_new/tokenizer_config.json',\n",
       " './model_save_new/special_tokens_map.json',\n",
       " './model_save_new/vocab.txt',\n",
       " './model_save_new/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save_new/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 101,083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"test_set.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Or what will happen in November?</td>\n",
       "      <td>Lvl32Ranger</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2016-07-05 17:03:37</td>\n",
       "      <td>You're living in a fantasy world. I hope to Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah cuz only democratic first ladies are trea...</td>\n",
       "      <td>zerowater02h</td>\n",
       "      <td>pics</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-10 00:55:07</td>\n",
       "      <td>I'm expecting her to be a more traditional Fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Nothing better than some good ol' fashioned fa...</td>\n",
       "      <td>bomba86</td>\n",
       "      <td>alaska</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08</td>\n",
       "      <td>2014-08-20 21:53:08</td>\n",
       "      <td>Great news. I really didn't want Alaska's long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Yea... Two compete separate things.. Might as ...</td>\n",
       "      <td>PMmeyourBM</td>\n",
       "      <td>verizon</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03</td>\n",
       "      <td>2015-03-11 00:30:52</td>\n",
       "      <td>Its a insurance policy that covers 100% after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>It's pretty ambiguous considering there are mu...</td>\n",
       "      <td>Baz-</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>2016-04-25 20:31:49</td>\n",
       "      <td>Okay, I'm sorry but I'm not quite 100% sure wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>It's my dick in a box!</td>\n",
       "      <td>slickeddie</td>\n",
       "      <td>WTF</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07</td>\n",
       "      <td>2015-07-30 22:43:27</td>\n",
       "      <td>Welcome to Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>You're assuming that there's progress.</td>\n",
       "      <td>MizerokRominus</td>\n",
       "      <td>wow</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12</td>\n",
       "      <td>2014-12-03 01:47:15</td>\n",
       "      <td>They could at least say something that shows o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Now imagine the xenomorph nagging you nonstop ...</td>\n",
       "      <td>Giftofgab24</td>\n",
       "      <td>xboxone</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03</td>\n",
       "      <td>2015-03-31 21:57:53</td>\n",
       "      <td>That would really suck if your wife spits acid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>they're all over facebook</td>\n",
       "      <td>johnny_pilgrim</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12</td>\n",
       "      <td>2013-12-31 20:20:55</td>\n",
       "      <td>Where are the Citizen Journalists in America?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>all good lol, I appreciate the correction</td>\n",
       "      <td>BluePiller1776</td>\n",
       "      <td>TheBluePill</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2016-07-21 18:45:36</td>\n",
       "      <td>NP, didn't mean to call you out as we're comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>would you want to?</td>\n",
       "      <td>Nicolay77</td>\n",
       "      <td>formula1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-02</td>\n",
       "      <td>2012-02-21 21:54:35</td>\n",
       "      <td>[](/a15) I couldn't do that even if I had 3 feet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>buzzzzzz</td>\n",
       "      <td>danthezombieking</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-17 03:01:14</td>\n",
       "      <td>She says \"systems buzzing!\" when I believe she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>To Europe!</td>\n",
       "      <td>Phrost_</td>\n",
       "      <td>hockey</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08</td>\n",
       "      <td>2014-08-27 14:36:51</td>\n",
       "      <td>Neither is putting teams in markets that don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah, a new CEO for one of the largest compani...</td>\n",
       "      <td>elitecommander</td>\n",
       "      <td>news</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12</td>\n",
       "      <td>2013-12-10 21:44:34</td>\n",
       "      <td>The fact that this is considered newsworthy me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Some types of equality are more equal than oth...</td>\n",
       "      <td>Draiko</td>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>2014-11-19 12:53:04</td>\n",
       "      <td>\"Equality\" Yet you align yourself with a group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Good thing a majority of anonymous speaks Arab...</td>\n",
       "      <td>30MinsToMoveYourCube</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>2015-01-10 14:55:33</td>\n",
       "      <td>Anonymous has announced that it will avenge th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>There's no proof so we know it's real!</td>\n",
       "      <td>waambulances</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-10</td>\n",
       "      <td>2013-10-26 23:50:00</td>\n",
       "      <td>So if there is a massive amount of fraud where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>This is obviously planted, that people actuall...</td>\n",
       "      <td>hazehk</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12</td>\n",
       "      <td>2015-12-08 15:41:57</td>\n",
       "      <td>Be careful... this could be a planted ad meant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>You know what I mean</td>\n",
       "      <td>RedditGotBoringYawn</td>\n",
       "      <td>wow</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-08 23:27:42</td>\n",
       "      <td>Had me at top DPs on a world boss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Get me a job and I'll be your bro.</td>\n",
       "      <td>ChesterHiggenbothum</td>\n",
       "      <td>pics</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03</td>\n",
       "      <td>2015-03-06 00:19:27</td>\n",
       "      <td>I work at an advertising agency filled with wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Filthy casuals</td>\n",
       "      <td>n0b0dya7a11</td>\n",
       "      <td>airsoft</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-10</td>\n",
       "      <td>2014-10-20 15:30:48</td>\n",
       "      <td>What's your airsoft pet peeve?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Upto 5\"</td>\n",
       "      <td>kailash_kk</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-30 03:38:56</td>\n",
       "      <td>Didn't get my draft tokens?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>MikeLeRoi</td>\n",
       "      <td>FarCry4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>2014-11-18 19:14:59</td>\n",
       "      <td>This is kind of a dumb question but can you pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Unless your a woman and want to work in a dayc...</td>\n",
       "      <td>t21spectre</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-07</td>\n",
       "      <td>2013-07-07 07:02:50</td>\n",
       "      <td>Pedophiles are attracted to positions they'll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>When GRRM planned a Jon-Arya-Tyrion love trian...</td>\n",
       "      <td>FireSteelMerica</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>2015-10-19 01:28:02</td>\n",
       "      <td>Remember this was back when there were only su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>oh come on !</td>\n",
       "      <td>m1000</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>2014-09-06 07:47:36</td>\n",
       "      <td>I had a girlfriend who always kissed with her ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Mitch Hedberg?</td>\n",
       "      <td>powercorruption</td>\n",
       "      <td>JoeRogan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-07</td>\n",
       "      <td>2013-07-15 16:32:45</td>\n",
       "      <td>The absolute king of the one liner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>All black people are criminals.</td>\n",
       "      <td>Firm_as_red_clay</td>\n",
       "      <td>bestofworldstar</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 17:06:54</td>\n",
       "      <td>College Kid Creates Sweatshirt That Says \"All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>He is also a young monkey, so there's that.</td>\n",
       "      <td>alwaysinthewoodshed</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>2016-03-03 06:43:28</td>\n",
       "      <td>Curious George isn't so much 'curious', as he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Either this or Gavin, Gus, Miles, and Kerry wo...</td>\n",
       "      <td>YaMeenz</td>\n",
       "      <td>roosterteeth</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07</td>\n",
       "      <td>2015-07-09 01:49:29</td>\n",
       "      <td>Burnie, Gus, Miles and Kerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101053</th>\n",
       "      <td>0</td>\n",
       "      <td>Want to see me shot gun this?</td>\n",
       "      <td>tlfalkenberg</td>\n",
       "      <td>BabyBumps</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11</td>\n",
       "      <td>2015-11-18 01:34:56</td>\n",
       "      <td>PASSED THE 1 HOUR!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101054</th>\n",
       "      <td>0</td>\n",
       "      <td>IDK man IDK</td>\n",
       "      <td>natlouwet</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07</td>\n",
       "      <td>2015-07-31 03:56:21</td>\n",
       "      <td>what the hell are we doing wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101055</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey, maybe they were just born in 1988!</td>\n",
       "      <td>AlwaysDefenestrated</td>\n",
       "      <td>news</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>2016-06-05 14:35:59</td>\n",
       "      <td>How strange, someone with 88 in their username...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101056</th>\n",
       "      <td>0</td>\n",
       "      <td>its always been good, IDK why its been overloo...</td>\n",
       "      <td>crazyweaselbob</td>\n",
       "      <td>Battlefield</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-16 23:07:21</td>\n",
       "      <td>Seems people are saying it's the new BAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101057</th>\n",
       "      <td>1</td>\n",
       "      <td>Please, everyone knows the only porn women nee...</td>\n",
       "      <td>Jessiebemessy</td>\n",
       "      <td>TrollYChromosome</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04</td>\n",
       "      <td>2015-04-04 17:49:24</td>\n",
       "      <td>MRW people think \"porn for women\" is that love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101058</th>\n",
       "      <td>1</td>\n",
       "      <td>Game breaking, Rito please disable Rengar</td>\n",
       "      <td>VladimirSnakeyes</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2016-07-08 01:43:40</td>\n",
       "      <td>TIL Rengar can jump to his own Ward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101059</th>\n",
       "      <td>0</td>\n",
       "      <td>Call the guys who made Iron Sky.</td>\n",
       "      <td>Robertotsexy98</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03</td>\n",
       "      <td>2014-03-18 23:43:21</td>\n",
       "      <td>That should be the next terminator. He goes ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101060</th>\n",
       "      <td>1</td>\n",
       "      <td>But it wasn't Islam that inspired him, it was ...</td>\n",
       "      <td>CJL13</td>\n",
       "      <td>KotakuInAction</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>2016-06-16 11:49:10</td>\n",
       "      <td>They do realize that a Muslim person of colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101061</th>\n",
       "      <td>0</td>\n",
       "      <td>This is not an apparent quality of yours</td>\n",
       "      <td>awkwardIRL</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>2016-08-22 00:45:41</td>\n",
       "      <td>i have no problem being wrong when i am.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101062</th>\n",
       "      <td>0</td>\n",
       "      <td>In a way yes, but she's being told if she lose...</td>\n",
       "      <td>Bay1Bri</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-13 16:14:38</td>\n",
       "      <td>They have too, there has to be something bigge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101063</th>\n",
       "      <td>0</td>\n",
       "      <td>It is considered part of North America</td>\n",
       "      <td>BozTheKing</td>\n",
       "      <td>Smite</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>2016-03-23 17:38:25</td>\n",
       "      <td>Isnt Mexico technically 'Central America'? or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101064</th>\n",
       "      <td>0</td>\n",
       "      <td>So when do I get to see OEL walking around in ...</td>\n",
       "      <td>boymayor</td>\n",
       "      <td>hockey</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05</td>\n",
       "      <td>2013-05-25 02:51:03</td>\n",
       "      <td>Multiple sources confirmed to FOX Sports Arizo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101065</th>\n",
       "      <td>0</td>\n",
       "      <td>Zack's solo album is gonna be amazing</td>\n",
       "      <td>PM_ME_WARIO_FANFICS</td>\n",
       "      <td>FULLCOMMUNISM</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-11 05:53:11</td>\n",
       "      <td>Me irl except I went from Conor Oberst to Zak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101066</th>\n",
       "      <td>0</td>\n",
       "      <td>Exactly what I came here to say :/</td>\n",
       "      <td>Ariboo02</td>\n",
       "      <td>quityourbullshit</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04</td>\n",
       "      <td>2015-04-13 01:27:11</td>\n",
       "      <td>You know what's sad? Eleven posts might quite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101067</th>\n",
       "      <td>0</td>\n",
       "      <td>If this happens, I wanna see Glenn go HAM on W...</td>\n",
       "      <td>dean16</td>\n",
       "      <td>thewalkingdead</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-11</td>\n",
       "      <td>2012-11-19 23:41:24</td>\n",
       "      <td>I just don't see that level of graphic detail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101068</th>\n",
       "      <td>1</td>\n",
       "      <td>But my dog is perfect and predictable!</td>\n",
       "      <td>Zlumberjack</td>\n",
       "      <td>FortCollins</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-05</td>\n",
       "      <td>2015-05-13 00:08:00</td>\n",
       "      <td>Leash your dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101069</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah because every position in the military is...</td>\n",
       "      <td>fuckhillaryinthebutt</td>\n",
       "      <td>pics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>2016-06-06 17:14:41</td>\n",
       "      <td>That dude laying on the beach trying to stuff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101070</th>\n",
       "      <td>0</td>\n",
       "      <td>English is easier than pretty much any Europea...</td>\n",
       "      <td>MotoTheBadMofo</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11</td>\n",
       "      <td>2015-11-06 16:58:16</td>\n",
       "      <td>No, English is one of the hardest languages to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh look, someone is bragging about having 5 mo...</td>\n",
       "      <td>Dirtydeedsinc</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11</td>\n",
       "      <td>2015-11-30 17:16:50</td>\n",
       "      <td>I have to find 145 more people to care about I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>1</td>\n",
       "      <td>what country do YOU live in?</td>\n",
       "      <td>ranhalt</td>\n",
       "      <td>Stargate</td>\n",
       "      <td>-6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-07 17:00:57</td>\n",
       "      <td>What country do you live in? I'm currently wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>1</td>\n",
       "      <td>Lucky you :&amp;lt; Apparently it's meant to make ...</td>\n",
       "      <td>kahrismatic</td>\n",
       "      <td>TrollXChromosomes</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07</td>\n",
       "      <td>2015-07-29 16:27:00</td>\n",
       "      <td>That... what? Why would you want to... what? T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>0</td>\n",
       "      <td>At the very least, a tasty snack.</td>\n",
       "      <td>Sinisteria</td>\n",
       "      <td>creepyPMs</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-05</td>\n",
       "      <td>2014-05-16 15:01:34</td>\n",
       "      <td>Hum. Humans are much more complicated than I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm gonna get up at 3PM and see what happens.</td>\n",
       "      <td>stev1212</td>\n",
       "      <td>ukpolitics</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>2016-06-19 16:43:09</td>\n",
       "      <td>I'm still mulling over whether it's worth stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101076</th>\n",
       "      <td>1</td>\n",
       "      <td>PYes after marginalized groups achieve eqality...</td>\n",
       "      <td>sicarim</td>\n",
       "      <td>scifi</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-07</td>\n",
       "      <td>2013-07-12 03:41:38</td>\n",
       "      <td>I disagree. The gender of the author is irrele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101077</th>\n",
       "      <td>1</td>\n",
       "      <td>Of course you would respond, who can say no to...</td>\n",
       "      <td>kelliwella</td>\n",
       "      <td>creepyPMs</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-19 04:44:52</td>\n",
       "      <td>Yeah, blame the technical issues.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101078</th>\n",
       "      <td>1</td>\n",
       "      <td>Wow... funny...</td>\n",
       "      <td>Tokenique</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-06</td>\n",
       "      <td>2014-06-10 22:03:16</td>\n",
       "      <td>they forgot to cancel the pizzas - someone sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101079</th>\n",
       "      <td>1</td>\n",
       "      <td>TIL feline leukemia is due to \"being kept in f...</td>\n",
       "      <td>sankafan</td>\n",
       "      <td>WTF</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 01:57:27</td>\n",
       "      <td>Cheetah taught to paint in Abu Dhabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101080</th>\n",
       "      <td>1</td>\n",
       "      <td>Gee Whiz I can't imagine Shussh kids Uncle Rus...</td>\n",
       "      <td>frackpot</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>2016-08-27 02:54:59</td>\n",
       "      <td>Guess Who Used BleachBit to Wipe Her Email Ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101081</th>\n",
       "      <td>1</td>\n",
       "      <td>But who would buy a $25 skin if it's only for ...</td>\n",
       "      <td>ma_vie_en_rose</td>\n",
       "      <td>bladeandsoul</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>2016-01-30 08:18:39</td>\n",
       "      <td>Plus, it's not uncommon for this sort of prici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101082</th>\n",
       "      <td>1</td>\n",
       "      <td>I bet you it's some random hackers</td>\n",
       "      <td>GreenyDev</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-28 08:13:10</td>\n",
       "      <td>Only all of SA trying to find out how long.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101083 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                            comment  \\\n",
       "0           0                   Or what will happen in November?   \n",
       "1           1  Yeah cuz only democratic first ladies are trea...   \n",
       "2           1  Nothing better than some good ol' fashioned fa...   \n",
       "3           0  Yea... Two compete separate things.. Might as ...   \n",
       "4           0  It's pretty ambiguous considering there are mu...   \n",
       "5           0                             It's my dick in a box!   \n",
       "6           0             You're assuming that there's progress.   \n",
       "7           0  Now imagine the xenomorph nagging you nonstop ...   \n",
       "8           1                          they're all over facebook   \n",
       "9           0          all good lol, I appreciate the correction   \n",
       "10          0                                 would you want to?   \n",
       "11          0                                           buzzzzzz   \n",
       "12          0                                         To Europe!   \n",
       "13          1  Yeah, a new CEO for one of the largest compani...   \n",
       "14          1  Some types of equality are more equal than oth...   \n",
       "15          1  Good thing a majority of anonymous speaks Arab...   \n",
       "16          1             There's no proof so we know it's real!   \n",
       "17          0  This is obviously planted, that people actuall...   \n",
       "18          0                               You know what I mean   \n",
       "19          0                 Get me a job and I'll be your bro.   \n",
       "20          1                                     Filthy casuals   \n",
       "21          1                                           \"Upto 5\"   \n",
       "22          0                                               Yes.   \n",
       "23          1  Unless your a woman and want to work in a dayc...   \n",
       "24          0  When GRRM planned a Jon-Arya-Tyrion love trian...   \n",
       "25          0                                       oh come on !   \n",
       "26          0                                     Mitch Hedberg?   \n",
       "27          1                    All black people are criminals.   \n",
       "28          0        He is also a young monkey, so there's that.   \n",
       "29          0  Either this or Gavin, Gus, Miles, and Kerry wo...   \n",
       "...       ...                                                ...   \n",
       "101053      0                      Want to see me shot gun this?   \n",
       "101054      0                                        IDK man IDK   \n",
       "101055      1            Hey, maybe they were just born in 1988!   \n",
       "101056      0  its always been good, IDK why its been overloo...   \n",
       "101057      1  Please, everyone knows the only porn women nee...   \n",
       "101058      1          Game breaking, Rito please disable Rengar   \n",
       "101059      0                   Call the guys who made Iron Sky.   \n",
       "101060      1  But it wasn't Islam that inspired him, it was ...   \n",
       "101061      0           This is not an apparent quality of yours   \n",
       "101062      0  In a way yes, but she's being told if she lose...   \n",
       "101063      0             It is considered part of North America   \n",
       "101064      0  So when do I get to see OEL walking around in ...   \n",
       "101065      0              Zack's solo album is gonna be amazing   \n",
       "101066      0                 Exactly what I came here to say :/   \n",
       "101067      0  If this happens, I wanna see Glenn go HAM on W...   \n",
       "101068      1             But my dog is perfect and predictable!   \n",
       "101069      1  Yeah because every position in the military is...   \n",
       "101070      0  English is easier than pretty much any Europea...   \n",
       "101071      1  Oh look, someone is bragging about having 5 mo...   \n",
       "101072      1                       what country do YOU live in?   \n",
       "101073      1  Lucky you :&lt; Apparently it's meant to make ...   \n",
       "101074      0                  At the very least, a tasty snack.   \n",
       "101075      0      I'm gonna get up at 3PM and see what happens.   \n",
       "101076      1  PYes after marginalized groups achieve eqality...   \n",
       "101077      1  Of course you would respond, who can say no to...   \n",
       "101078      1                                    Wow... funny...   \n",
       "101079      1  TIL feline leukemia is due to \"being kept in f...   \n",
       "101080      1  Gee Whiz I can't imagine Shussh kids Uncle Rus...   \n",
       "101081      1  But who would buy a $25 skin if it's only for ...   \n",
       "101082      1                 I bet you it's some random hackers   \n",
       "\n",
       "                      author            subreddit  score  ups  downs     date  \\\n",
       "0                Lvl32Ranger  SandersForPresident      1    1      0  2016-07   \n",
       "1               zerowater02h                 pics      3   -1     -1  2016-11   \n",
       "2                    bomba86               alaska      2    2      0  2014-08   \n",
       "3                 PMmeyourBM              verizon      2    2      0  2015-03   \n",
       "4                       Baz-               asoiaf      6    6      0  2016-04   \n",
       "5                 slickeddie                  WTF      1    1      0  2015-07   \n",
       "6             MizerokRominus                  wow      3    3      0  2014-12   \n",
       "7                Giftofgab24              xboxone      3    3      0  2015-03   \n",
       "8             johnny_pilgrim           Journalism      1    1      0  2013-12   \n",
       "9             BluePiller1776          TheBluePill      6    6      0  2016-07   \n",
       "10                 Nicolay77             formula1      2    2      0  2012-02   \n",
       "11          danthezombieking            Overwatch      5   -1     -1  2016-11   \n",
       "12                   Phrost_               hockey      3    3      0  2014-08   \n",
       "13            elitecommander                 news      2    2      0  2013-12   \n",
       "14                    Draiko                funny      1    1      0  2014-11   \n",
       "15      30MinsToMoveYourCube            worldnews      1    1      0  2015-01   \n",
       "16              waambulances    explainlikeimfive      0    0      0  2013-10   \n",
       "17                    hazehk           conspiracy      4    4      0  2015-12   \n",
       "18       RedditGotBoringYawn                  wow      1   -1     -1  2016-12   \n",
       "19       ChesterHiggenbothum                 pics      2    2      0  2015-03   \n",
       "20               n0b0dya7a11              airsoft      6    6      0  2014-10   \n",
       "21                kailash_kk                 FIFA      1    1      0  2016-09   \n",
       "22                 MikeLeRoi              FarCry4      6    6      0  2014-11   \n",
       "23                t21spectre           TheRedPill      1    1      0  2013-07   \n",
       "24           FireSteelMerica               asoiaf     10   10      0  2015-10   \n",
       "25                     m1000            AskReddit      3    3      0  2014-09   \n",
       "26           powercorruption             JoeRogan      2    2      0  2013-07   \n",
       "27          Firm_as_red_clay      bestofworldstar      2   -1     -1  2016-10   \n",
       "28       alwaysinthewoodshed       Showerthoughts      1    1      0  2016-03   \n",
       "29                   YaMeenz         roosterteeth      3    3      0  2015-07   \n",
       "...                      ...                  ...    ...  ...    ...      ...   \n",
       "101053          tlfalkenberg            BabyBumps      2    2      0  2015-11   \n",
       "101054             natlouwet            AskReddit      1    1      0  2015-07   \n",
       "101055   AlwaysDefenestrated                 news      5    5      0  2016-06   \n",
       "101056        crazyweaselbob          Battlefield      1   -1     -1  2016-11   \n",
       "101057         Jessiebemessy     TrollYChromosome    113  113      0  2015-04   \n",
       "101058      VladimirSnakeyes      leagueoflegends      3    3      0  2016-07   \n",
       "101059        Robertotsexy98            AskReddit      1    1      0  2014-03   \n",
       "101060                 CJL13       KotakuInAction      3    3      0  2016-06   \n",
       "101061            awkwardIRL          nottheonion      1    1      0  2016-08   \n",
       "101062               Bay1Bri             politics      1   -1     -1  2016-10   \n",
       "101063            BozTheKing                Smite      2    2      0  2016-03   \n",
       "101064              boymayor               hockey     23   23      0  2013-05   \n",
       "101065   PM_ME_WARIO_FANFICS        FULLCOMMUNISM      6   -1     -1  2016-12   \n",
       "101066              Ariboo02     quityourbullshit    -41  -41      0  2015-04   \n",
       "101067                dean16       thewalkingdead     28   28      0  2012-11   \n",
       "101068           Zlumberjack          FortCollins      3    3      0  2015-05   \n",
       "101069  fuckhillaryinthebutt                 pics      0    0      0  2016-06   \n",
       "101070        MotoTheBadMofo            AskReddit      2    2      0  2015-11   \n",
       "101071         Dirtydeedsinc        todayilearned      4    4      0  2015-11   \n",
       "101072               ranhalt             Stargate     -6   -1     -1  2016-11   \n",
       "101073           kahrismatic    TrollXChromosomes     19   19      0  2015-07   \n",
       "101074            Sinisteria            creepyPMs      4    4      0  2014-05   \n",
       "101075              stev1212           ukpolitics      6    6      0  2016-06   \n",
       "101076               sicarim                scifi      1    1      0  2013-07   \n",
       "101077            kelliwella            creepyPMs      5    5      0  2016-09   \n",
       "101078             Tokenique        todayilearned      1    1      0  2014-06   \n",
       "101079              sankafan                  WTF      1   -1     -1  2016-10   \n",
       "101080              frackpot             politics      1    1      0  2016-08   \n",
       "101081        ma_vie_en_rose         bladeandsoul      1    1      0  2016-01   \n",
       "101082             GreenyDev             Adelaide      4    4      0  2016-09   \n",
       "\n",
       "                created_utc                                     parent_comment  \n",
       "0       2016-07-05 17:03:37  You're living in a fantasy world. I hope to Go...  \n",
       "1       2016-11-10 00:55:07  I'm expecting her to be a more traditional Fir...  \n",
       "2       2014-08-20 21:53:08  Great news. I really didn't want Alaska's long...  \n",
       "3       2015-03-11 00:30:52  Its a insurance policy that covers 100% after ...  \n",
       "4       2016-04-25 20:31:49  Okay, I'm sorry but I'm not quite 100% sure wh...  \n",
       "5       2015-07-30 22:43:27                                   Welcome to Japan  \n",
       "6       2014-12-03 01:47:15  They could at least say something that shows o...  \n",
       "7       2015-03-31 21:57:53  That would really suck if your wife spits acid...  \n",
       "8       2013-12-31 20:20:55      Where are the Citizen Journalists in America?  \n",
       "9       2016-07-21 18:45:36  NP, didn't mean to call you out as we're comin...  \n",
       "10      2012-02-21 21:54:35  [](/a15) I couldn't do that even if I had 3 feet.  \n",
       "11      2016-11-17 03:01:14  She says \"systems buzzing!\" when I believe she...  \n",
       "12      2014-08-27 14:36:51  Neither is putting teams in markets that don't...  \n",
       "13      2013-12-10 21:44:34  The fact that this is considered newsworthy me...  \n",
       "14      2014-11-19 12:53:04  \"Equality\" Yet you align yourself with a group...  \n",
       "15      2015-01-10 14:55:33  Anonymous has announced that it will avenge th...  \n",
       "16      2013-10-26 23:50:00  So if there is a massive amount of fraud where...  \n",
       "17      2015-12-08 15:41:57  Be careful... this could be a planted ad meant...  \n",
       "18      2016-12-08 23:27:42                  Had me at top DPs on a world boss  \n",
       "19      2015-03-06 00:19:27  I work at an advertising agency filled with wo...  \n",
       "20      2014-10-20 15:30:48                     What's your airsoft pet peeve?  \n",
       "21      2016-09-30 03:38:56                        Didn't get my draft tokens?  \n",
       "22      2014-11-18 19:14:59  This is kind of a dumb question but can you pi...  \n",
       "23      2013-07-07 07:02:50  Pedophiles are attracted to positions they'll ...  \n",
       "24      2015-10-19 01:28:02  Remember this was back when there were only su...  \n",
       "25      2014-09-06 07:47:36  I had a girlfriend who always kissed with her ...  \n",
       "26      2013-07-15 16:32:45                The absolute king of the one liner.  \n",
       "27      2016-10-16 17:06:54  College Kid Creates Sweatshirt That Says \"All ...  \n",
       "28      2016-03-03 06:43:28  Curious George isn't so much 'curious', as he ...  \n",
       "29      2015-07-09 01:49:29                       Burnie, Gus, Miles and Kerry  \n",
       "...                     ...                                                ...  \n",
       "101053  2015-11-18 01:34:56                             PASSED THE 1 HOUR!!!!!  \n",
       "101054  2015-07-31 03:56:21                   what the hell are we doing wrong  \n",
       "101055  2016-06-05 14:35:59  How strange, someone with 88 in their username...  \n",
       "101056  2016-11-16 23:07:21           Seems people are saying it's the new BAR  \n",
       "101057  2015-04-04 17:49:24  MRW people think \"porn for women\" is that love...  \n",
       "101058  2016-07-08 01:43:40                TIL Rengar can jump to his own Ward  \n",
       "101059  2014-03-18 23:43:21  That should be the next terminator. He goes ba...  \n",
       "101060  2016-06-16 11:49:10  They do realize that a Muslim person of colour...  \n",
       "101061  2016-08-22 00:45:41           i have no problem being wrong when i am.  \n",
       "101062  2016-10-13 16:14:38  They have too, there has to be something bigge...  \n",
       "101063  2016-03-23 17:38:25  Isnt Mexico technically 'Central America'? or ...  \n",
       "101064  2013-05-25 02:51:03  Multiple sources confirmed to FOX Sports Arizo...  \n",
       "101065  2016-12-11 05:53:11  Me irl except I went from Conor Oberst to Zak ...  \n",
       "101066  2015-04-13 01:27:11  You know what's sad? Eleven posts might quite ...  \n",
       "101067  2012-11-19 23:41:24  I just don't see that level of graphic detail ...  \n",
       "101068  2015-05-13 00:08:00                                    Leash your dog.  \n",
       "101069  2016-06-06 17:14:41  That dude laying on the beach trying to stuff ...  \n",
       "101070  2015-11-06 16:58:16  No, English is one of the hardest languages to...  \n",
       "101071  2015-11-30 17:16:50  I have to find 145 more people to care about I...  \n",
       "101072  2016-11-07 17:00:57  What country do you live in? I'm currently wat...  \n",
       "101073  2015-07-29 16:27:00  That... what? Why would you want to... what? T...  \n",
       "101074  2014-05-16 15:01:34  Hum. Humans are much more complicated than I h...  \n",
       "101075  2016-06-19 16:43:09  I'm still mulling over whether it's worth stay...  \n",
       "101076  2013-07-12 03:41:38  I disagree. The gender of the author is irrele...  \n",
       "101077  2016-09-19 04:44:52                  Yeah, blame the technical issues.  \n",
       "101078  2014-06-10 22:03:16  they forgot to cancel the pizzas - someone sti...  \n",
       "101079  2016-10-18 01:57:27               Cheetah taught to paint in Abu Dhabi  \n",
       "101080  2016-08-27 02:54:59  Guess Who Used BleachBit to Wipe Her Email Ser...  \n",
       "101081  2016-01-30 08:18:39  Plus, it's not uncommon for this sort of prici...  \n",
       "101082  2016-09-28 08:13:10        Only all of SA trying to find out how long.  \n",
       "\n",
       "[101083 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.dropna(subset=[\"comment\"]).comment.values\n",
    "labels = df.dropna(subset=[\"comment\"]).label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /gpfs/software/Anaconda3/lib/python3.6/site-packages (2.0.8)\r\n",
      "Requirement already satisfied: pyyaml in /gpfs/software/Anaconda3/lib/python3.6/site-packages (from keras) (3.12)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /gpfs/software/Anaconda3/lib/python3.7/site-packages (from keras) (1.18.1)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /gpfs/software/Anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /gpfs/software/Anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101080/101080 [00:04<00:00, 23029.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pad our input tokens\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in tqdm(input_ids):\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 101,080 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3159/3159 [07:17<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "actuals=[]\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "    preds.append(list(np.argmax(predictions[i], axis=1).flatten()))\n",
    "    actuals.append(list(true_labels[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "preds = list(itertools.chain(*preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = list(itertools.chain(*actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.6206964780372"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(actuals,preds)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761840396479036"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Macro F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(actuals, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from string import ascii_uppercase\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confm = confusion_matrix(actuals, preds,normalize='true')\n",
    "\n",
    "columns = [\"Sarcastic\",\"Normal\"]\n",
    "\n",
    "\n",
    "df_cm = DataFrame(confm, index=columns, columns=columns)\n",
    "\n",
    "ax = sn.heatmap(df_cm, cmap='Oranges', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-env",
   "language": "python",
   "name": "bert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
